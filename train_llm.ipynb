{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "uc1N-tadO81Y",
      "metadata": {
        "id": "uc1N-tadO81Y"
      },
      "source": [
        "# Step 1. Corpus Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6d54be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a6d54be",
        "outputId": "bfa6a983-e0e8-440f-cb30-f15ea0e7caf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Working dir: /content\n"
          ]
        }
      ],
      "source": [
        "import os, re, io, json, html, glob, shutil, tarfile, zipfile, urllib.request, random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "LANG_TAG = \"tt\"\n",
        "VOCABS   = [8000, 16000, 32000]\n",
        "#CHOSEN_VOCAB = 16000\n",
        "BLOCK_SIZE   = 512\n",
        "N_LAYER = 8\n",
        "N_HEAD  = 8\n",
        "N_EMBD  = 512\n",
        "LR      = 3e-4\n",
        "NUM_EPOCHS = 1   #\n",
        "SEED = 42\n",
        "\n",
        "# dir\n",
        "ROOT      = Path.cwd()\n",
        "DATA_RAW  = ROOT/\"data/raw\"\n",
        "DATA_CLEAN= ROOT/\"data/clean\"\n",
        "DATA_META = ROOT/\"data/meta\"\n",
        "TOK_DIR   = ROOT/\"tokenizer_bpe\"\n",
        "CKPT_DIR  = ROOT/f\"checkpoints/{LANG_TAG}-gpt-small\"\n",
        "\n",
        "for p in [DATA_RAW, DATA_CLEAN, DATA_META, TOK_DIR, CKPT_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "print(\"Working dir:\", ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FJzoxWwQPeWZ",
      "metadata": {
        "id": "FJzoxWwQPeWZ"
      },
      "source": [
        "## Corpus: Tatar mixed 2015 (Leipzig Corpora Collection [link text](https://downloads.wortschatz-leipzig.de/corpora/tat_mixed_2015_1M.tar.gz))\n",
        "**Size: ~1 million sentences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50906000",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50906000",
        "outputId": "91516f79-6bd6-441e-fcbc-a6be3edb9697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: https://downloads.wortschatz-leipzig.de/corpora/tat_mixed_2015_1M.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1863578971.py:22: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  t.extractall(to_dir)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected files: 7\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "RAW_DIR = DATA_RAW\n",
        "URLS = [\n",
        "    \"https://downloads.wortschatz-leipzig.de/corpora/tat_mixed_2015_1M.tar.gz\",\n",
        "]\n",
        "\n",
        "TMP = RAW_DIR/\"_tmp\"\n",
        "TMP.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _download(url: str, out_path: Path):\n",
        "    print(\"Downloading:\", url)\n",
        "    with urllib.request.urlopen(url) as r, open(out_path, \"wb\") as f:\n",
        "        shutil.copyfileobj(r, f)\n",
        "\n",
        "def _extract(path: Path, to_dir: Path):\n",
        "    name = path.name.lower()\n",
        "    if name.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(path, \"r\") as z:\n",
        "            z.extractall(to_dir)\n",
        "    elif name.endswith(\".tar.gz\") or name.endswith(\".tgz\"):\n",
        "        with tarfile.open(path, \"r:gz\") as t:\n",
        "            t.extractall(to_dir)\n",
        "    elif name.endswith(\".tar\"):\n",
        "        with tarfile.open(path, \"r\") as t:\n",
        "            t.extractall(to_dir)\n",
        "\n",
        "for url in URLS:\n",
        "    f = TMP/url.split(\"/\")[-1]\n",
        "    _download(url, f)\n",
        "    _extract(f, TMP)\n",
        "\n",
        "# collect txt/tsv\n",
        "collected = 0\n",
        "for p in TMP.rglob(\"*\"):\n",
        "    if p.is_file() and p.suffix.lower() in [\".txt\", \".tsv\"]:\n",
        "        tgt = RAW_DIR/p.name\n",
        "        if tgt.exists():\n",
        "            i = 1\n",
        "            while (RAW_DIR/f\"{p.stem}_{i}{p.suffix}\").exists():\n",
        "                i += 1\n",
        "            tgt = RAW_DIR/f\"{p.stem}_{i}{p.suffix}\"\n",
        "        shutil.copy2(p, tgt)\n",
        "        collected += 1\n",
        "\n",
        "print(\"Collected files:\", collected)\n",
        "shutil.rmtree(TMP, ignore_errors=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tIzCQXqBQGJ6",
      "metadata": {
        "id": "tIzCQXqBQGJ6"
      },
      "source": [
        "# Cleaning , Filtering & Deduplication\n",
        "\n",
        "Custom cleaning functions were implemented:\n",
        "\n",
        "basic_clean() — removes HTML entities, redundant spaces and repeated punctuation.\n",
        "\n",
        "too_numeric() — filters out sentences with more than 60 % digits.\n",
        "\n",
        "likely_tatarish() — keeps only Tatar sentences based on unique Cyrillic letters (Ә, Ө, Ү, Җ, Ң, Һ).\n",
        "\n",
        "Then each sentence is normalized (lower-cased, extra spaces removed) and checked for uniqueness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mbg2AMrz4QKz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbg2AMrz4QKz",
        "outputId": "ea2b7581-6547-474c-ffdc-6b5eacedd67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw lines: 1000000 | kept (pre-dedup): 999367 | unique: 999197\n",
            "Saved 999197 lines -> data/clean/tt_clean.txt\n"
          ]
        }
      ],
      "source": [
        "import re, html\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_RAW = Path(\"data/raw\")\n",
        "DATA_CLEAN = Path(\"data/clean\")\n",
        "LANG_TAG = \"tt\"\n",
        "\n",
        "def basic_clean(s: str) -> str:\n",
        "    s = html.unescape(s)\n",
        "    s = s.replace(\"\\r\", \"\\n\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    s = re.sub(r\"([!?.,…])\\1{2,}\", r\"\\1\\1\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def too_numeric(s: str, max_ratio=0.6) -> bool:\n",
        "    toks = [ch for ch in s if ch.isalnum()]\n",
        "    if not toks:\n",
        "        return True\n",
        "    digits = sum(ch.isdigit() for ch in toks)\n",
        "    return digits / len(toks) > max_ratio\n",
        "\n",
        "CYRIL = re.compile(r\"[А-Яа-яЁёӘәӨөҮүҖҗҢңҺһ]\")\n",
        "TATAR_UNIQUE = re.compile(r\"[ӘәӨөҮүҖҗҢңҺһ]\")\n",
        "\n",
        "def likely_tatarish(s: str, hard_ratio=0.55, soft_ratio=0.35) -> bool:\n",
        "    letters = [ch for ch in s if ch.isalpha()]\n",
        "    if not letters:\n",
        "        return False\n",
        "    cyr = sum(1 for ch in letters if CYRIL.match(ch))\n",
        "    ratio = cyr / len(letters)\n",
        "    return (ratio >= hard_ratio) or (ratio >= soft_ratio and TATAR_UNIQUE.search(s))\n",
        "\n",
        "cands = list(DATA_RAW.rglob(\"tat_mixed_2015_1M-sentences.txt\"))\n",
        "assert cands, \"not found tat_mixed_2015_1M-sentences.txt\"\n",
        "src = cands[0]\n",
        "\n",
        "out_path = DATA_CLEAN / f\"{LANG_TAG}_clean.txt\"\n",
        "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "kept, total, pre = [], 0, 0\n",
        "with open(src, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fin:\n",
        "    for line in fin:\n",
        "        total += 1\n",
        "        parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "        sent = basic_clean(parts[-1])         # 永远取最后一列\n",
        "        if len(sent) < 3:\n",
        "            continue\n",
        "        if too_numeric(sent, max_ratio=0.6):\n",
        "            continue\n",
        "        if not likely_tatarish(sent):\n",
        "            continue\n",
        "        kept.append(sent); pre += 1\n",
        "\n",
        "# Deduplication\n",
        "seen, uniq = set(), []\n",
        "for s in kept:\n",
        "    key = re.sub(r\"\\s+\", \" \", s.lower()).strip()\n",
        "    if key not in seen:\n",
        "        seen.add(key)\n",
        "        uniq.append(s)\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for s in uniq:\n",
        "        fout.write(s + \"\\n\")\n",
        "\n",
        "print(f\"Raw lines: {total} | kept (pre-dedup): {pre} | unique: {len(uniq)}\")\n",
        "print(f\"Saved {len(uniq)} lines -> {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MHGEAWwtQvwD",
      "metadata": {
        "id": "MHGEAWwtQvwD"
      },
      "source": [
        "# Corpus Statistics (To sum up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_js3uVIYzh26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_js3uVIYzh26",
        "outputId": "800a82b0-0f56-4c50-9b49-bb191fe5d422"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_lines': 999197,\n",
              " 'total_chars': 104168737,\n",
              " 'avg_len': 104.25245171873014,\n",
              " 'median_len': 95.0}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "def corpus_stats(lines):\n",
        "    lengths = [len(x) for x in lines]\n",
        "    return {\n",
        "        \"num_lines\": len(lines),\n",
        "        \"total_chars\": int(sum(lengths)),\n",
        "        \"avg_len\": float(np.mean(lengths)) if lengths else 0,\n",
        "        \"median_len\": float(np.median(lengths)) if lengths else 0,\n",
        "    }\n",
        "\n",
        "stats = corpus_stats(uniq)\n",
        "DATA_META.mkdir(parents=True, exist_ok=True)\n",
        "with open(DATA_META/\"stats.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(stats, f, ensure_ascii=False, indent=2)\n",
        "stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aIyKhMWtTY5w",
      "metadata": {
        "id": "aIyKhMWtTY5w"
      },
      "source": [
        "# Step 2. Training a Byte-level BPE from zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f74c2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43f74c2a",
        "outputId": "578abd30-2fe1-4ae1-bfa1-c75a7082eb74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{8000: '/content/tokenizer_bpe/bpe_8000/tokenizer.json',\n",
              " 16000: '/content/tokenizer_bpe/bpe_16000/tokenizer.json',\n",
              " 32000: '/content/tokenizer_bpe/bpe_32000/tokenizer.json'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from pathlib import Path\n",
        "CLEAN_FILE = Path(\"data/clean/tt_clean.txt\")\n",
        "assert CLEAN_FILE.exists()\n",
        "\n",
        "def train_bpe_tokenizer(vocab_size: int, save_dir: Path):\n",
        "    files = [str(CLEAN_FILE)]\n",
        "    tok = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "    tok.pre_tokenizer = ByteLevel()\n",
        "    trainer = BpeTrainer(\n",
        "        vocab_size=vocab_size,\n",
        "        special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"],\n",
        "        min_frequency=2,\n",
        "        show_progress=True,\n",
        "    )\n",
        "    tok.train(files, trainer)\n",
        "    tok.post_processor = TemplateProcessing(\n",
        "        single=\"<bos> $A <eos>\",\n",
        "        pair=\"<bos> $A <eos> <bos> $B <eos>\",\n",
        "        special_tokens=[\n",
        "            (\"<bos>\", tok.token_to_id(\"<bos>\")),\n",
        "            (\"<eos>\", tok.token_to_id(\"<eos>\")),\n",
        "        ],\n",
        "    )\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    tok.save(str(save_dir/\"tokenizer.json\"))\n",
        "    return save_dir/\"tokenizer.json\"\n",
        "\n",
        "TOK_RUNS = {}\n",
        "for vs in VOCABS:\n",
        "    sd = TOK_DIR/f\"bpe_{vs}\"\n",
        "    path = train_bpe_tokenizer(vs, sd)\n",
        "    TOK_RUNS[vs] = str(path)\n",
        "TOK_RUNS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brBgEMMgztrm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brBgEMMgztrm",
        "outputId": "b1bf49e7-7a15-436a-ef55-30bb53a22abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab | size | avg_tokens_per_sent | tail_lowfreq_ratio\n",
            "(8000, 8000, 26.31, 0.025)\n",
            "(16000, 16000, 23.46, 0.0088)\n",
            "(32000, 32000, 21.68, 0.0028)\n"
          ]
        }
      ],
      "source": [
        "# ===== 2.2 分词器对比指标：Avg tokens / sentence + 低频占比（近似）=====\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "def sample_lines(path, n=3000):\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    if len(uniq) <= n: return uniq\n",
        "    idx = rng.choice(len(uniq), n, replace=False)\n",
        "    return [uniq[i] for i in idx]\n",
        "\n",
        "def eval_tokenizer(tok_json_path, sample):\n",
        "    tok = Tokenizer.from_file(tok_json_path)\n",
        "    # 计算 avg tokens / sentence\n",
        "    lens=[]\n",
        "    for s in sample:\n",
        "        enc = tok.encode(s)\n",
        "        lens.append(len(enc.ids))\n",
        "    avg_tok = float(np.mean(lens)) if lens else 0.0\n",
        "\n",
        "    # 低频近似：取 vocab 末端（假设是低频合并形成的长子词），统计句内使用率\n",
        "    vocab_size = tok.get_vocab_size()\n",
        "    tail_ids = set(range(max(0, vocab_size-1000), vocab_size))\n",
        "    tail_hits=0\n",
        "    total=0\n",
        "    for s in sample:\n",
        "        enc = tok.encode(s)\n",
        "        for i in enc.ids:\n",
        "            total += 1\n",
        "            if i in tail_ids: tail_hits += 1\n",
        "    tail_ratio = tail_hits / max(1,total)\n",
        "    return avg_tok, tail_ratio, vocab_size\n",
        "\n",
        "sample = sample_lines(CLEAN_FILE, n=3000)\n",
        "tok_stats = []\n",
        "for vs, path in TOK_RUNS.items():\n",
        "    avg_tok, tail_ratio, size = eval_tokenizer(path, sample)\n",
        "    tok_stats.append((vs, size, round(avg_tok,2), round(tail_ratio,4)))\n",
        "tok_stats = sorted(tok_stats)\n",
        "print(\"Vocab | size | avg_tokens_per_sent | tail_lowfreq_ratio\")\n",
        "for row in tok_stats:\n",
        "    print(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LdGrBLalaY5_",
      "metadata": {
        "id": "LdGrBLalaY5_"
      },
      "source": [
        "**Choice of Final Tokenizer (16k Vocabulary)**\n",
        "- Increasing the vocabulary from 8k → 16k significantly reduces the average token length (≈11%)  \n",
        "  and lowers the share of low-frequency tokens, improving segmentation efficiency.\n",
        "- However, further expansion to 32k provides only marginal gains  \n",
        "  while doubling the embedding table size and memory cost.\n",
        "- Therefore, **the 16k vocabulary offers the best balance**  \n",
        "  between segmentation compactness and model simplicity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oNr2-OcYzz_y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "7a9c9c99d0714c9487c7c786793502eb",
            "970ac52fb3844645a4d873f8d63cb53f",
            "2ed8ad0345ba4c409c35ee12e51734be",
            "3ce755d04cc84b38a3fadde01ed94f78",
            "de539b4868a547bd9a184b0054c09904",
            "cbbb684fd935440c961ebd2ebeab7b2b",
            "d33228435e35405d9c26dd4bd4b12550",
            "171101bbb1534371a5028afaab1a3ab4",
            "4073db26ff8b45d19b1757825ce35e63",
            "0eabaf5781c74bf29125b187bf01ef5b",
            "e443e5cbe86d47c0bb44f4bd80d1a111",
            "353c2bdfea7141cdb83d0ea12b70f9e3",
            "cd34a06421ea424cba7d264ba1a30444",
            "6aacdf60dd30462eae12a6c8ba9e8293",
            "037dbc2171574e4fbcf262d452d34807",
            "38b990be859744f2883632c1f1822fab",
            "1910bdba315b40b8bd1a25157b00dbe8",
            "7477d3a0ac0046369bf3ecae64b45555",
            "1a4ee563953449feb4eb03283f007170",
            "2e8bcfc7b3a04ee4bcbd93bbc9101650",
            "65ce14f8be8e4059bad6fb8ca8093911",
            "c9301e967c0b453eb61bd873c2b46872",
            "b938780082b542409880a303fcf44473",
            "5f70f432c9574031a99a2b997cc77a72",
            "b5359b08fb524d93a29163de7a426674",
            "19ec46f21f3545aeb43ce59573686b24",
            "58babbf5eb1a48f387e124a7844e521f",
            "cfbd8e4fc5e24b8fb8050eeaf40742be",
            "2caaed53c6c349289a3c17df9257de5a",
            "bedcce17f61e4020915c20962046d16c",
            "0d05ce77b7e54f51b6b6659b93990205",
            "d0c0d2ea7d8849848970bdf1bef2f135",
            "28ec64749b034120910370d88e6b9371",
            "1b00ee032f9a46068fa8b55fc6f32f9e",
            "d386549f4846445991453d87a241faec",
            "51ff8ae21e504bf3bcabb79d057c3838",
            "8dfa6d84055c4d4c8c999000da2e5ff1",
            "7908f1bc00774b6f99ab47c262bc3800",
            "de5690371e9649239e3c4579b3ba44cf",
            "0a04ac1103b94269a19a5bf0970e7847",
            "45506989ee9c4793a5616c3b0276be2a",
            "038d2c17fe2441159c881f68e37db468",
            "47edfc6383ec4280aab0af4bd4d18e54",
            "5407ffe78ad448dab6e6e99e5a8841c3",
            "2445778a376846429185f8fb749c4ad5",
            "eea80daee8484212b77b83047feaac81",
            "6f8cf134ccb6458a8202fb10a2896b7a",
            "2a0d9af919f84405a751e6b499835beb",
            "bf99fde71eb743619398e8038903069f",
            "92a0dea1d584496e9bb6331b29f16f84",
            "957fa60de0314c378b21fb2c5932daed",
            "dbbe7ce9d70c4bbc8e0085a19956b6af",
            "9e52e109b4684842994100dd3cec44ba",
            "5463140bfdb247c29855a1bfeea18c50",
            "b79e7b5b2fa947678727c820b60e9d78"
          ]
        },
        "id": "oNr2-OcYzz_y",
        "outputId": "a9d20494-17ce-48be-ac56-656ec3360ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size = 16000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a9c9c99d0714c9487c7c786793502eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "353c2bdfea7141cdb83d0ea12b70f9e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/979213 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b938780082b542409880a303fcf44473",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/19984 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b00ee032f9a46068fa8b55fc6f32f9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/979213 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2445778a376846429185f8fb749c4ad5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/19984 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(44935, 917)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "CHOSEN_VOCAB = 16000\n",
        "CHOSEN_TOK_PATH = TOK_RUNS[CHOSEN_VOCAB]\n",
        "tok_fast = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=CHOSEN_TOK_PATH,\n",
        "    bos_token=\"<bos>\", eos_token=\"<eos>\", unk_token=\"<unk>\", pad_token=\"<pad>\"\n",
        ")\n",
        "print(\"Vocab size =\", tok_fast.vocab_size)\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": str(CLEAN_FILE)})\n",
        "split = dataset[\"train\"].train_test_split(test_size=0.02, seed=SEED)\n",
        "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "def tokenize(examples):\n",
        "    return tok_fast(examples[\"text\"])\n",
        "\n",
        "tokenized_train = train_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "tokenized_val   = val_ds.map(tokenize,   batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "def group_texts(examples, block_size=BLOCK_SIZE):\n",
        "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = (len(concatenated[\"input_ids\"]) // block_size) * block_size\n",
        "    res = {k: [t[i:i+block_size] for i in range(0, total_length, block_size)]\n",
        "           for k, t in concatenated.items()}\n",
        "    res[\"labels\"] = res[\"input_ids\"].copy()\n",
        "    return res\n",
        "\n",
        "lm_train = tokenized_train.map(group_texts, batched=True)\n",
        "lm_val   = tokenized_val.map(group_texts,   batched=True)\n",
        "len(lm_train), len(lm_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QcJJbY_Ac0_o",
      "metadata": {
        "id": "QcJJbY_Ac0_o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "de7588e5",
      "metadata": {
        "id": "de7588e5"
      },
      "source": [
        "## Step3. Training a decoder-only gpt model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CkLgzf9pco2d",
      "metadata": {
        "id": "CkLgzf9pco2d"
      },
      "source": [
        "- Define a GPT2-like decoder-only model from scratch using the Transformers library.\n",
        "\n",
        "\n",
        "- Use the self-trained Byte-level BPE tokenizer with a 16k vocabulary.\n",
        "\n",
        "\n",
        "- Load the cleaned corpus and split it into 98% training and 2% validation subsets.\n",
        "\n",
        "\n",
        "- Implement language modeling data chunking with a block size of 512 tokens.\n",
        "\n",
        "\n",
        "- Use a network configuration of 8 layers × 8 attention heads × 512 hidden dimensions.\n",
        "\n",
        "\n",
        "- Enable gradient checkpointing, bf16 mixed precision (if supported by the GPU), and a warmup ratio to stabilize the early training phase;\n",
        "then run the training and save the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1C1F_n0AXJ9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "f1C1F_n0AXJ9",
        "outputId": "d3ca697e-c3ff-4787-9d8f-d73387053087"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1026897649.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2809' max='2809' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2809/2809 1:02:56, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>6.268600</td>\n",
              "      <td>6.128636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>5.576500</td>\n",
              "      <td>5.510249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>5.257400</td>\n",
              "      <td>5.221183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>5.031800</td>\n",
              "      <td>4.995275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>4.830700</td>\n",
              "      <td>4.823112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>4.716000</td>\n",
              "      <td>4.684253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>4.585400</td>\n",
              "      <td>4.566749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>4.492900</td>\n",
              "      <td>4.476864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>4.419700</td>\n",
              "      <td>4.402056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.346300</td>\n",
              "      <td>4.338768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>4.295300</td>\n",
              "      <td>4.284287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>4.238900</td>\n",
              "      <td>4.241837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>4.211500</td>\n",
              "      <td>4.209074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>4.188800</td>\n",
              "      <td>4.191027</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to: /content/checkpoints/tt-gpt-small\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=tok_fast.vocab_size,\n",
        "    n_positions=BLOCK_SIZE,\n",
        "    n_ctx=BLOCK_SIZE,\n",
        "    n_layer=N_LAYER,\n",
        "    n_head=N_HEAD,\n",
        "    n_embd=N_EMBD,\n",
        "    resid_pdrop=0.0, embd_pdrop=0.0, attn_pdrop=0.0,\n",
        "    bos_token_id=tok_fast.bos_token_id, eos_token_id=tok_fast.eos_token_id\n",
        ")\n",
        "model = GPT2LMHeadModel(config).to(device)\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tok_fast, mlm=False)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(CKPT_DIR),\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LR,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=torch.cuda.is_available(),\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args,\n",
        "    train_dataset=lm_train, eval_dataset=lm_val,\n",
        "    data_collator=collator, tokenizer=tok_fast\n",
        ")\n",
        "\n",
        "train_out = trainer.train()\n",
        "trainer.save_model(str(CKPT_DIR))\n",
        "tok_fast.save_pretrained(str(CKPT_DIR))\n",
        "print(\"Saved to:\", CKPT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stUXkAhbc8_M",
      "metadata": {
        "id": "stUXkAhbc8_M"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UnbA9fWHLLEP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "UnbA9fWHLLEP",
        "outputId": "ec8eb3f1-461b-4baa-844f-6d92847ddcdb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [115/115 00:20]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.190962314605713, 'eval_runtime': 20.7888, 'eval_samples_per_second': 44.11, 'eval_steps_per_second': 5.532, 'epoch': 1.0}\n",
            "Perplexity: 66.08635623645245\n"
          ]
        }
      ],
      "source": [
        "from math import exp\n",
        "eval_res = trainer.evaluate(eval_dataset=lm_val)\n",
        "print(eval_res)\n",
        "if \"eval_loss\" in eval_res:\n",
        "    print(\"Perplexity:\", exp(eval_res[\"eval_loss\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Th7K647ihBnE",
      "metadata": {
        "id": "Th7K647ihBnE"
      },
      "source": [
        "# Pushing to huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KKj42XKYSQwr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294,
          "referenced_widgets": [
            "48aa84f7b9164e5fa713b6240be78f0d",
            "d104b321346a460dac9fde2a7692899a",
            "ed4a4a7ef0c846a5af8fc821f7f8306a",
            "176430ca8028448f8f3e39e771cfaed6",
            "3169d9c49ef74b8ca8e1f9abbbf361a8",
            "1cf50cfaf01b492e85a8a376102b0c12",
            "0c1ea49634724f7dacbdc1b87698a81b",
            "a74be053c59e4f21a2f2c577a36498d3",
            "66340a1a815443b4a0aa8dea31786732",
            "beef575fa39442e68217484fe61d457c",
            "78d6c64fba754eecb783ea9c13a36787",
            "9879b3002c4f41a7b9b2facfc3fe4003",
            "0407ed1dfc6c4b10b8cbc006d926338f",
            "12789911a4ab42168f4d4be00ece8d18",
            "afc6e9567738432dbf81ce42f18d357d",
            "7489fcb6fd6141539b9b22fd7f73877a",
            "aa35a1e9174c4162a35d396e89f2a569",
            "0230355d1982405699e87e370800655e",
            "22a857e9f4ca4682aed4291a867d2042",
            "2ae30c301e1140e5bc7ffdc509002c8e",
            "af88bef63734442cb263c7e3c9861fcd",
            "bb3bd44efba14d4693e570b061d769c6",
            "6add2c928e3c4587b750a799569174b1",
            "bc0fd4f21ff5475b81a38d821794eda8",
            "cf6bfdfdc2db4c438a8c3ce590493afb",
            "7aef10ccc9d645d7b590bd1ce5f9c22d",
            "265fad0d532a48c3abb0302bdb5774f0",
            "2285bc5289454248be3921205078c70e",
            "6e8abde98a204c3ab13121535fb8f034",
            "edb280857377480ba09c28ce5153c0b8",
            "586c521c90eb408b9794f9883dfd6a5c",
            "286dacc72f2a40858f0f535e8f515c80",
            "f7bba9f9a463453b83bd8c7aa4690e97",
            "5d501457cae14b81a961c75d062cd19b",
            "cc39687808c04aba8686ce8c71f2c934",
            "8e37b457b43b447a9dffe0ed51f9e4a6",
            "41962e6fd5b149058c9401e71ccf1f3f",
            "718ee187a43940c9b6991671d4bdbc3a",
            "d69b758475454ae79e131ff8df0c4111",
            "43155bb395d5443da9c372030f2f37af",
            "144dcb7c552141f5a77f1e0e5162277c",
            "bec6ed52f56b49d097670dd18666a981",
            "9d3cc2a1e6854e5d9c3d55b2c4def2b3",
            "95c032b444804a37bc3b356a5ded21aa"
          ]
        },
        "id": "KKj42XKYSQwr",
        "outputId": "6d56ea3b-cb36-4315-8562-e5c03ad3aef3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48aa84f7b9164e5fa713b6240be78f0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9879b3002c4f41a7b9b2facfc3fe4003",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6add2c928e3c4587b750a799569174b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d501457cae14b81a961c75d062cd19b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...8ud2_zx/model.safetensors:   0%|          |  553kB /  135MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pushed: https://huggingface.co/xinyuema/tt-gpt-small2\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(str(CKPT_DIR))\n",
        "model.config._name_or_path = REPO_ID\n",
        "\n",
        "tok_fast.push_to_hub(REPO_ID, commit_message=\"Add tokenizer\")\n",
        "model.push_to_hub(REPO_ID, commit_message=\"Add model weights and config\")\n",
        "\n",
        "print(\"Pushed:\", f\"https://huggingface.co/{REPO_ID}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}